services:
#
  # frontend:
  #   build:
  #     context: ./naviSafe-frontend
  #     dockerfile: Dockerfile
  #   container_name: frontend
  #   ports:
  #     - "5173:5173"
  #   environment:
  #     - NODE_ENV=development
  #   volumes:
  #     - ./naviSafe-frontend:/app
  #     - /app/node_modules
  #   command: npm run dev
  #   depends_on:
  #     - backend

  # backend:
  #   build:
  #     context: ./naviSafe-backend
  #     dockerfile: Dockerfile
  #   container_name: backend
  #   ports:
  #     - "8080:8080"
  #   command: java -jar app.jar

  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper

  producer:
    build:
      context: ./dataflow
    container_name: producer
    depends_on:
      - kafka
    # environment:
    #   API_KEY: ${API_KEY}
    volumes:
      - ./dataflow/producer.py:/app/preprocessing/producer.py
    working_dir: /app
    command: sleep infinity

  weather-consumer:
    build:
      context: ./dataflow
    container_name: weather-consumer
    depends_on:
      - kafka
    volumes:
      - ./dataflow/consumers:/app/consumers
    working_dir: /app/consumers
    command: python3 weather_consumer.py

  traffic-consumer:
    build:
      context: ./dataflow
    container_name: traffic-consumer
    depends_on:
      - kafka
    volumes:
      - ./dataflow/consumers:/app/consumers
    working_dir: /app/consumers
    command: python3 traffic_consumer.py

  subway-consumer:
    build:
      context: ./dataflow
    container_name: subway-consumer
    depends_on:
      - kafka
    volumes:
      - ./dataflow/consumers:/app/consumers
    working_dir: /app/consumers
    command: python3 subway_consumer.py

  
  preprocessing:
    image: jupyter/pyspark-notebook:latest
    container_name: preprocessing
    environment:
      - PYTHONPATH=/app
    depends_on:
      - mysql
      - airflow-scheduler
      - airflow-webserver
    volumes:
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow/utils:/app/utils
    working_dir: /app/preprocessing
    env_file:
      - ./dataflow/.env
    command: bash -c "pip install --no-cache-dir pyspark==3.5.5 redis mysql-connector-python && python outbreak_preprocessing.py"
    restart: unless-stopped

  linkinfo:
    build:
      context: ./dataflow
    container_name: linkinfo
    depends_on:
      - redis
      - mysql
      #- preprocessing
    volumes:
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow/utils:/app/utils
    working_dir: /app/preprocessing
    environment:
      - PYTHONPATH=/app
    env_file:
      - ./dataflow/.env
    command: bash -c "pip install --no-cache-dir redis mysql-connector-python && python linkinfo_worker.py && tail -f /dev/null"
    restart: unless-stopped

  spark-master:
    image: bitnami/spark:3.5.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"   # Spark Web UI
      - "7077:7077"   # Spark Master

  spark-worker:
    image: bitnami/spark:3.5.5
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master

  jupyter:
    image: jupyter/pyspark-notebook
    container_name: spark-jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./dataflow/notebooks:/home/jovyan/work
    depends_on:
      - spark-master
      - spark-worker

  redis:
    image: redis:7.2.0
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - ./dataflow/utils:/app/utils
    #command: python3 redis.py
    restart: unless-stopped

  redis-client:
    image: python:3.11-slim
    container_name: redis-client
    depends_on:
      - redis
    volumes:
      - ./dataflow/utils:/app/utils
    working_dir: /app/utils
    command: bash -c "pip install --no-cache-dir redis && python check_redis.py"

  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3307:3306"          # 로컬 3306 → 컨테이너 3306
    environment:
      MYSQL_ROOT_PASSWORD: rootpass
      #MYSQL_DATABASE: toy_project
      MYSQL_USER: user
      MYSQL_PASSWORD: userpass
    volumes:
      - ./dataflow/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 5s
      retries: 5
    depends_on:
      - weather-consumer
      - traffic-consumer
      - subway-consumer
    # volumes:
    #   - ./mysql-data:/var/lib/mysql

  airflow-init:
    build:
      context: ./dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-init
    depends_on:
      mysql:
        condition: service_healthy
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+pymysql://user:userpass@mysql:3306/airflow_db
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    command: >
          bash -c "airflow db migrate && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin && echo 'Airflow 초기화 완료!'"
    volumes:
      - ./dataflow/airflow/dags:/opt/airflow/dags
      - ./dataflow/airflow/logs:/opt/airflow/logs
      - ./dataflow/airflow/plugins:/opt/airflow/plugins
      - ./dataflow:/opt/airflow/dataflow
      - ./dataflow/preprocessing:/app/preprocessing

  airflow-scheduler:
    build:
      context: ./dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      #AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/plugins
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
    volumes:
      - ./dataflow/airflow/dags:/opt/airflow/dags
      - ./dataflow/airflow/plugins:/opt/airflow/plugins
      - ./dataflow/airflow/logs:/opt/airflow/logs
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow:/opt/airflow/dataflow
      - ./dataflow/utils:/app/utils
      - ./dataflow/wait-for-it.sh:/wait-for-it.sh
    working_dir: /opt/airflow
    env_file:
      - ./dataflow/.env
    depends_on:
      mysql:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    #entrypoint: ["/wait-for-it.sh", "mysql", "3306", "--"]
    command: >
      bash -c "airflow scheduler"
  
  airflow-webserver:
    build:
      context: ./dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
      AIRFLOW__WEBSERVER__RBAC: 'true'
    volumes:
      - ./dataflow/airflow/dags:/opt/airflow/dags
      - ./dataflow/airflow/plugins:/opt/airflow/plugins
      - ./dataflow/airflow/logs:/opt/airflow/logs
      - ./dataflow:/opt/airflow/dataflow
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow/wait-for-it.sh:/wait-for-it.sh
    working_dir: /opt/airflow
    depends_on:
      mysql:
        condition: service_healthy
      airflow-scheduler:
        condition: service_started
    ports:
      - "8081:8080"
    #entrypoint: ["/wait-for-it.sh", "mysql", "3306", "--"]
    command: >
      bash -c "airflow webserver"

  subway-api:
    build:
      context: ./dataflow
    container_name: transit
    depends_on:
      - kafka        # Kafka 연동이 필요하면
    environment:
      SEOUL_SUBWAY_ARRIVAL_API_KEY: ${SEOUL_SUBWAY_ARRIVAL_API_KEY}
      SEOUL_TRANSIT_API_KEY: ${SEOUL_TRANSIT_API_KEY}
    volumes:
      - ./dataflow/app.py:/app/app.py       # 로컬 app.py → 컨테이너 /app/app.py
    working_dir: /app
    ports:
      - "5000:5000"               # Flask API 외부 노출
    command: python3 app.py
    restart: unless-stopped


networks:
  default:
    driver: bridge
