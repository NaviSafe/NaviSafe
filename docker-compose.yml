services:
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper

  producer:
    build:
      context: ./dataflow
    container_name: producer
    depends_on:
      - kafka
    # environment:
    #   API_KEY: ${API_KEY}
    volumes:
      - ./dataflow:/app/dataflow
    environment:
      - PYTHONPATH=/app/dataflow
    working_dir: /app
    command: sleep infinity

  weather-consumer:
    build:
      context: ./dataflow
    container_name: weather-consumer
    depends_on:
      - kafka
    volumes:
      - ./dataflow/consumers:/app/consumers
    working_dir: /app/consumers
    command: python3 weather_consumer.py

  traffic-consumer:
    build:
      context: ./dataflow
    container_name: traffic-consumer
    depends_on:
      - kafka
    volumes:
      - ./dataflow/consumers:/app/consumers
    working_dir: /app/consumers
    command: python3 traffic_consumer.py

  subway-consumer:
    build:
      context: ./dataflow
    container_name: subway-consumer
    depends_on:
      - kafka
    volumes:
      - ./dataflow/consumers:/app/consumers
    working_dir: /app/consumers
    command: python3 subway_consumer.py

  
  preprocessing:
    image: jupyter/pyspark-notebook:latest
    container_name: preprocessing
    environment:
      - PYTHONPATH=/app
    depends_on:
      - mysql
      - airflow-scheduler
      - airflow-webserver
    volumes:
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow/utils:/app/utils
    working_dir: /app/preprocessing
    env_file:
      - ./dataflow/.env
    command: bash -c "pip install --no-cache-dir pyspark==3.5.5 redis mysql-connector-python && python outbreak_streaming.py"
    restart: unless-stopped

  emergency-streaming:
    image: jupyter/pyspark-notebook:latest
    container_name: emergency-streaming
    environment:
      - PYTHONPATH=/app
    depends_on:
      - kafka
      - redis
      - mysql
    volumes:
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow/utils:/app/utils
    working_dir: /app/preprocessing
    env_file:
      - ./dataflow/.env
    command: >
      bash -c "
      pip install --no-cache-dir pyspark==3.5.5 redis mysql-connector-python &&
      python emergency_streaming.py
      "
    restart: unless-stopped

  # linkinfo:
  #   build:
  #     context: ./dataflow
  #   container_name: linkinfo
  #   depends_on:
  #     - redis
  #     - mysql
  #     #- preprocessing
  #   volumes:
  #     - ./dataflow/preprocessing:/app/preprocessing
  #     - ./dataflow/utils:/app/utils
  #   working_dir: /app/preprocessing
  #   environment:
  #     - PYTHONPATH=/app
  #   env_file:
  #     - ./dataflow/.env
  #   command: bash -c "pip install --no-cache-dir redis mysql-connector-python && python linkinfo_worker.py && tail -f /dev/null"
  #   restart: unless-stopped

  spark-master:
    image: bitnami/spark:3.5.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8082:8080"   # Spark Web UI
      - "7077:7077"   # Spark Master

  spark-worker:
    image: bitnami/spark:3.5.5
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master

  # jupyter:
  #   image: jupyter/pyspark-notebook
  #   container_name: spark-jupyter
  #   ports:
  #     - "8888:8888"
  #   volumes:
  #     - ./dataflow/notebooks:/home/jovyan/work
  #   depends_on:
  #     - spark-master
  #     - spark-worker

  redis:
    image: redis:7.2.0
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - ./dataflow/utils:/app/utils
    #command: python3 redis.py
    restart: unless-stopped

  # redis-client:
  #   image: python:3.11-slim
  #   container_name: redis-client
  #   depends_on:
  #     - redis
  #   volumes:
  #     - ./dataflow/utils:/app/utils
  #   working_dir: /app/utils
  #   command: bash -c "pip install --no-cache-dir redis && python check_redis.py"

  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3307:3306"          # 로컬 3306 → 컨테이너 3306
    env_file:
      - ./dataflow/.env
    # environment:
    #   MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    #   MYSQL_DATABASE: ${MYSQL_DATABASE}
    #   MYSQL_USER: ${MYSQL_USER}
    #   MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    volumes:
      - ./dataflow/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 5s
      retries: 5
    depends_on:
      - weather-consumer
      - traffic-consumer
      - subway-consumer
    # volumes:
    #   - ./mysql-data:/var/lib/mysql

  airflow-init:
    build:
      context: ./dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-init
    depends_on:
      mysql:
        condition: service_healthy
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+pymysql://user:userpass@mysql:3306/airflow_db
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - PYTHONPATH=/opt/airflow/dataflow
    command: >
          bash -c "airflow db migrate && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin && echo 'Airflow 초기화 완료!'"
    volumes:
      - ./dataflow/airflow/dags:/opt/airflow/dags
      - ./dataflow/airflow/logs:/opt/airflow/logs
      - ./dataflow/airflow/plugins:/opt/airflow/plugins
      - ./dataflow:/opt/airflow/dataflow
      - ./dataflow/preprocessing:/app/preprocessing

  airflow-scheduler:
    build:
      context: ./dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      #AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/plugins
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
      AIRFLOW__METRICS__STATSD_ON: "True"
      AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"  
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: "airflow"
      PYTHONPATH : /opt/airflow/dataflow
    volumes:
      - ./dataflow/airflow/dags:/opt/airflow/dags
      - ./dataflow/airflow/plugins:/opt/airflow/plugins
      - ./dataflow/airflow/logs:/opt/airflow/logs
      - ./dataflow/tests:/opt/airflow/tests
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow:/opt/airflow/dataflow
      - ./dataflow/utils:/app/utils
      - ./dataflow/wait-for-it.sh:/wait-for-it.sh
    working_dir: /opt/airflow
    env_file:
      - ./dataflow/.env
    depends_on:
      mysql:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    #entrypoint: ["/wait-for-it.sh", "mysql", "3306", "--"]
    command: >
      bash -c "airflow scheduler"
  
  airflow-webserver:
    build:
      context: ./dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
      AIRFLOW__METRICS__STATSD_ON: "True"
      AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: "airflow"
      AIRFLOW__WEBSERVER__RBAC: 'true'
      PYTHONPATH : /opt/airflow/dataflow
    volumes:
      - ./dataflow/airflow/dags:/opt/airflow/dags
      - ./dataflow/airflow/plugins:/opt/airflow/plugins
      - ./dataflow/airflow/logs:/opt/airflow/logs
      - ./dataflow:/opt/airflow/dataflow
      - ./dataflow/preprocessing:/app/preprocessing
      - ./dataflow/wait-for-it.sh:/wait-for-it.sh
    working_dir: /opt/airflow
    depends_on:
      mysql:
        condition: service_healthy
      airflow-scheduler:
        condition: service_started
    ports:
      - "8081:8080"
    #entrypoint: ["/wait-for-it.sh", "mysql", "3306", "--"]
    command: >
      bash -c "airflow webserver"

  subway-api:
    build:
      context: ./dataflow
    container_name: transit
    depends_on:
      - kafka        # Kafka 연동이 필요하면
    environment:
      SEOUL_SUBWAY_ARRIVAL_API_KEY: ${SEOUL_SUBWAY_ARRIVAL_API_KEY}
      SEOUL_TRANSIT_API_KEY: ${SEOUL_TRANSIT_API_KEY}
    volumes:
      - ./dataflow/app.py:/app/app.py       # 로컬 app.py → 컨테이너 /app/app.py
    working_dir: /app
    ports:
      - "5000:5000"               # Flask API 외부 노출
    command: python3 app.py
    restart: unless-stopped
    
  frontend:
    build:
      context: ./naviSafe-frontend
      dockerfile: Dockerfile
    container_name: frontend
    ports:
      - "5173:5173"
    environment:
      - NODE_ENV=development
    volumes:
      - ./naviSafe-frontend:/app
      - /app/node_modules
    command: npm run dev
    depends_on:
      - backend

  backend:
    build:
      context: ./naviSafe-backend
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "8080:8080"
    command: java -jar app.jar
    depends_on:
      mysql:
        condition: service_healthy

  # subway-streaming:
  #   build:
  #     context: ./dataflow
  #     dockerfile: Dockerfile.subway
  #   container_name: subway-streaming
  #   working_dir: /app
  #   depends_on:
  #     - kafka
  #     - redis
  #     - spark-master
  #     - spark-worker
  #     - airflow-webserver
  #   environment:
  #     - PYTHONPATH=/app
  #   restart: unless-stopped
  
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml # Prometheus 설정 파일 매핑
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    ports:
      - "9090:9090"
    depends_on:
      - kafka
      - redis
      - mysql
      - airflow-webserver
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana # Grafana 데이터 영속화
    depends_on:
      - prometheus
    restart: unless-stopped

  redis-exporter:
    image: oliver006/redis_exporter
    container_name: redis-exporter
    command:
      - "--redis.addr=redis://redis:6379"
    ports:
      - "9121:9121"
    depends_on:
      - redis
    restart: unless-stopped

  mysql-exporter:
    image: prom/mysqld-exporter:latest
    container_name: mysql-exporter
    command:
      - "--mysqld.username=exporter:userpass"
      - "--mysqld.address=mysql:3306"
    ports:
      - "9104:9104"
    depends_on:
      mysql:
        condition: service_healthy
    restart: always

  statsd-exporter:
    image: prom/statsd-exporter
    container_name: statsd-exporter 
    ports:
      - "9125:9125/udp" # StatsD 수신 포트
      - "9102:9102"   # Prometheus 메트릭 노출 포트


volumes:
  grafana_data:


networks:
  default:
    driver: bridge