services:
  airflow-init:
    build:
      context: ../dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-init
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+pymysql://user:userpass@mysql:3306/airflow_db
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - PYTHONPATH=/opt/airflow/dataflow
    command: >
          bash -c "airflow db migrate && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin && echo 'Airflow 초기화 완료!'"
    volumes:
      - ../dataflow/airflow/dags:/opt/airflow/dags
      - ../dataflow/airflow/logs:/opt/airflow/logs
      - ../dataflow/airflow/plugins:/opt/airflow/plugins
      - ../dataflow:/opt/airflow/dataflow
      - ../dataflow/preprocessing:/app/preprocessing

  airflow-scheduler:
    build:
      context: ../dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      #AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/plugins
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
      AIRFLOW__METRICS__STATSD_ON: "True"
      AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"  
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: "airflow"
      PYTHONPATH : /opt/airflow/dataflow
    volumes:
      - ../dataflow/airflow/dags:/opt/airflow/dags
      - ../dataflow/airflow/plugins:/opt/airflow/plugins
      - ../dataflow/airflow/logs:/opt/airflow/logs
      - ../dataflow/tests:/opt/airflow/tests
      - ../dataflow/preprocessing:/app/preprocessing
      - ../dataflow:/opt/airflow/dataflow
      - ../dataflow/utils:/app/utils
      - ../dataflow/wait-for-it.sh:/wait-for-it.sh
    working_dir: /opt/airflow
    env_file:
      - ../dataflow/.env
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    #entrypoint: ["/wait-for-it.sh", "mysql", "3306", "--"]
    command: >
      bash -c "airflow scheduler"
  
  airflow-webserver:
    build:
      context: ../dataflow
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
      AIRFLOW__METRICS__STATSD_ON: "True"
      AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: "airflow"
      AIRFLOW__WEBSERVER__RBAC: 'true'
      PYTHONPATH : /opt/airflow/dataflow
    volumes:
      - ../dataflow/airflow/dags:/opt/airflow/dags
      - ../dataflow/airflow/plugins:/opt/airflow/plugins
      - ../dataflow/airflow/logs:/opt/airflow/logs
      - ../dataflow:/opt/airflow/dataflow
      - ../dataflow/preprocessing:/app/preprocessing
      - ../dataflow/wait-for-it.sh:/wait-for-it.sh
    working_dir: /opt/airflow
    depends_on:
      airflow-scheduler:
        condition: service_started
    ports:
      - "8081:8080"
    #entrypoint: ["/wait-for-it.sh", "mysql", "3306", "--"]
    command: >
      bash -c "airflow webserver"

networks:
  data-platform:
    external: true